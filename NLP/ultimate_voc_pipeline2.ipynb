{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# df = pd.read_excel(\"raw data/LG-A9K_REVIEWS.xlsx\")\n",
    "\n",
    "df = pd.read_csv(\"raw data/LG-A9K_REVIEWS.csv\")\n",
    "# df = pd.read_csv(\"raw data/LG-A9K_REVIEWS.csv\", encoding='cp1252')\n",
    "df.insert(0, 'ID', range(0 + len(df)))\n",
    "df['REVIEW'] = df['TITLE'] + \". \" + df['CONTENT']\n",
    "df['REVIEW'] = df['REVIEW'].replace(np.nan, '')\n",
    "df = df.drop(columns=['TITLE', 'CONTENT'])\n",
    "\n",
    "print(df.head())\n",
    "# Remove empty strings after colon and add space before Pros and Cons\n",
    "df['REVIEW'] = df['REVIEW'].apply(\n",
    "    lambda x: x.replace(\":   \", \":\").replace(\":  \", \":\").replace(\": \", \":\").replace(\":\\n\\n\", \": \").replace(\":\\n\",\n",
    "                                                                                                           \": \").replace(\n",
    "        \"\\t\", \"\").replace(\"\\n-\", \"\").replace(\"\\n \", \"\\n\"))\n",
    "# df['REVIEW'][0]\n",
    "\n",
    "\n",
    "df['REVIEW']\n",
    "\n",
    "\n",
    "# Separate REVIEW into PARAGRAPHS\n",
    "def separate_paragraphs(review):\n",
    "    para_list = []\n",
    "    paragraphs = review.split('\\n\\n')\n",
    "    para_list.extend(paragraphs)\n",
    "    return para_list\n",
    "\n",
    "\n",
    "df['PARAGRAPHS'] = df['REVIEW'].apply(lambda x: separate_paragraphs(x))\n",
    "\n",
    "# Remove empty strings from PARAGRAPHS\n",
    "# df['PARAGRAPHS'] = df['PARAGRAPHS'].apply(lambda x: list(filter(None, x)))\n",
    "# df['PARAGRAPHS'][0]\n",
    "\n",
    "df['PARAGRAPHS'][0]\n",
    "# Drop df columns: REVIEW\n",
    "df = df.drop(columns=['RETAILER', 'PRODUCT', 'POST_DATE', 'REVIEWER_NAME', 'REVIEW'])\n",
    "\n",
    "# Make PARAGRAPHS list to string\n",
    "# df['PARAGRAPHS'] = df['PARAGRAPHS'].agg(lambda x: ','.join(map(str, x)))\n",
    "df.head()\n",
    "\n",
    "# df.insert(1, 'P_ID', range(0 + len(df)))\n",
    "\n",
    "\n",
    "# explode the PARAGRAPHS Column\n",
    "df = df.explode('PARAGRAPHS')\n",
    "df = df.reset_index(drop=True)\n",
    "# remove empty rows from PARAGRAPHS\n",
    "df_paragraphs = df[df['PARAGRAPHS'] != '']\n",
    "\n",
    "df_paragraphs\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "from scipy.special import softmax\n",
    "\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "def polarity_scores_roberta(review):\n",
    "    encoded_text = tokenizer(review, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "    encoded_text\n",
    "    # print(encoded_text)\n",
    "    output = model(**encoded_text)\n",
    "    # output\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    scores\n",
    "    scores_dict = {\n",
    "        'negative': scores[0],\n",
    "        'neutral': scores[1],\n",
    "        'positive': scores[2]\n",
    "    }\n",
    "    return scores_dict\n",
    "\n",
    "\n",
    "# polarity_scores_roberta(review)\n",
    "df_paragraphs.insert(1, 'P_ID', range(0 + len(df_paragraphs)))\n",
    "\n",
    "df_paragraphs\n",
    "\n",
    "res = {}\n",
    "for i, row in tqdm(df_paragraphs.iterrows(), total=len(df_paragraphs)):\n",
    "    try:\n",
    "        text = row['PARAGRAPHS']\n",
    "        myid = row['P_ID']\n",
    "        # vader_results = sia.polarity_scores(text)\n",
    "        roberta_result = polarity_scores_roberta(text)\n",
    "        res[myid] = {**roberta_result}\n",
    "    except RuntimeError:\n",
    "        print(f'Broke for id {myid}')\n",
    "\n",
    "results_df = pd.DataFrame(res).T\n",
    "results_df = results_df.reset_index().rename(columns={'index': 'P_ID'})\n",
    "results_df = results_df.merge(df_paragraphs, how='left')\n",
    "results_df.head()\n",
    "# Keywords Extraction\n",
    "# added below since giving a certificate verification error.\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# copy results_df to keywords_df\n",
    "keywords_df = results_df.copy()\n",
    "keywords_df['PARAGRAPHS'][524]\n",
    "import string, re\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "for i in range(0, len(keywords_df['PARAGRAPHS'])):\n",
    "    keywords_df['PARAGRAPHS'][i] = keywords_df['PARAGRAPHS'][i].translate(str.maketrans('', '', string.punctuation))\n",
    "    keywords_df['PARAGRAPHS'][i] = keywords_df['PARAGRAPHS'][i].replace('\\n', '. ')\n",
    "    keywords_df['PARAGRAPHS'][i] = keywords_df['PARAGRAPHS'][i].lower()\n",
    "    # keywords_df['PARAGRAPHS'][i] = re.sub(\"['\\\"]\",\"\",keywords_df['PARAGRAPHS'][i])\n",
    "    for j in re.findall('\"([^\"]*)\"', keywords_df['PARAGRAPHS'][i]):\n",
    "        keywords_df['PARAGRAPHS'][i] = keywords_df['PARAGRAPHS'][i].replace('\"{}\"'.format(j), j.replace(' ', '_'))\n",
    "\n",
    "keywords_df['PARAGRAPHS'][524]\n",
    "\n",
    "# word_tokenize paragraphs\n",
    "\n",
    "keywords_df['KEYWORD'] = keywords_df['PARAGRAPHS'].apply(lambda x: word_tokenize(x))\n",
    "keywords_df['KEYWORD']\n",
    "english_stopwords = stopwords.words('english')\n",
    "for i in range(0, len(keywords_df['KEYWORD'])):\n",
    "    keywords_df['KEYWORD'][i] = [w for w in keywords_df['KEYWORD'][i] if w.lower() not in english_stopwords]\n",
    "\n",
    "# remove duplicate keywords_df['KEYWORD']\n",
    "keywords_df['KEYWORD'] = keywords_df['KEYWORD'].apply(lambda x: list(dict.fromkeys(x)))\n",
    "keywords_df['KEYWORD']\n",
    "# apply rake_nltk_var.extract_keywords_from_text to each row in PARAGRAPHS\n",
    "# keywords_df['KEYWORDS'] = keywords_df['PARAGRAPHS'].apply(lambda x: rake_nltk_var.extract_keywords_from_text(x))\n",
    "# keywords_df['KEYWORDS'] = keywords_df['KEYWORDS'].apply(lambda x: rake_nltk_var.get_ranked_phrases())\n",
    "# keywords_df\n",
    "keywords_df['PARAGRAPHS'][524]\n",
    "from rake_nltk import Metric\n",
    "\n",
    "r = Rake(include_repeated_phrases=False,\n",
    "         min_length=2,\n",
    "         ranking_metric=Metric.WORD_DEGREE)\n",
    "keywords = []\n",
    "for i in range(0, len(keywords_df['PARAGRAPHS'])):\n",
    "    keyword = r.extract_keywords_from_text(keywords_df['PARAGRAPHS'][i])\n",
    "    keyword = r.get_ranked_phrases()\n",
    "    keywords.append(keyword)\n",
    "keywords_df['KEYWORDS'] = keywords\n",
    "len(keywords)\n",
    "keywords_df['KEYWORD'][524]\n",
    "keywords_df['KEYWORDS'][524]\n",
    "# save to csv\n",
    "keywords_df.to_csv('keywords_df_RAKE.csv', encoding='utf-8-sig', index=False)\n",
    "\n",
    "from pke.unsupervised import YAKE\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "document = \"Machine learning (ML) is the study of computer algorithms that improve automatically through experience. It is seen as a subset of artificial intelligence.\"\n",
    "\n",
    "# 1. Create YAKE keyword extractor\n",
    "extractor = YAKE()\n",
    "\n",
    "# 2. Load document\n",
    "extractor.load_document(input=document,\n",
    "                        language='en',\n",
    "                        normalization=None)\n",
    "\n",
    "# 3. Generate candidate 1-gram and 2-gram keywords\n",
    "stoplist = stopwords.words('english')\n",
    "extractor.candidate_selection(n=2, stoplist=stoplist)\n",
    "\n",
    "# 4. Calculate scores for the candidate keywords\n",
    "extractor.candidate_weighting(window=2,\n",
    "                              stoplist=stoplist,\n",
    "                              use_stems=False)\n",
    "\n",
    "# 5. Select 10 highest ranked keywords\n",
    "# Remove redundant keywords with similarity above 80%\n",
    "key_phrases = extractor.get_n_best(n=10, threshold=0.8)\n",
    "print(key_phrases)\n",
    "import yake\n",
    "\n",
    "document = \"Machine learning (ML) is the study of computer algorithms that improve automatically through experience. It is seen as a subset of artificial intelligence.\"\n",
    "kw_extractor = yake.KeywordExtractor()\n",
    "keywords = kw_extractor.extract_keywords(document)\n",
    "\n",
    "for kw in keywords:\n",
    "    print(kw)\n",
    "\n",
    "language = \"en\"\n",
    "max_ngram_size = 3\n",
    "deduplication_threshold = 0.9\n",
    "deduplication_algo = 'seqm'\n",
    "windowSize = 1\n",
    "numOfKeywords = 20\n",
    "\n",
    "custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold,\n",
    "                                            dedupFunc=deduplication_algo, windowsSize=windowSize, top=numOfKeywords,\n",
    "                                            features=None)\n",
    "keywords = custom_kw_extractor.extract_keywords(text)\n",
    "\n",
    "for kw in keywords:\n",
    "    print(kw)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
